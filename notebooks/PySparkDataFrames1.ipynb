{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "948c2c88-5f4f-4bff-8c89-7b5354b3e6e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m823.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a804c060-2ba8-4349-98f6-382685470d80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a62cdf-70bc-4679-8436-02dab28c70b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpanda\u001b[39;00m \n\u001b[1;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline_retail_combined\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'panda'"
     ]
    }
   ],
   "source": [
    "import panda \n",
    "pd.read_csv('online_retail_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8486bb2-182d-47fa-a895-fff52a97d5a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d1ec4a1-dd2b-4843-b19d-2e091cd442ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24e4291a-5e6a-45c6-a885-a6bfba5e8f4b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Country</th>\n",
       "      <th>fiscal_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067366</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067367</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067368</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067369</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067370</th>\n",
       "      <td>581587</td>\n",
       "      <td>POST</td>\n",
       "      <td>POSTAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1067371 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Invoice StockCode                          Description  Quantity  \\\n",
       "0        489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1        489434    79323P                   PINK CHERRY LIGHTS        12   \n",
       "2        489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3        489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4        489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "...         ...       ...                                  ...       ...   \n",
       "1067366  581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "1067367  581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "1067368  581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "1067369  581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "1067370  581587      POST                              POSTAGE         1   \n",
       "\n",
       "                 InvoiceDate  Price  Customer ID         Country fiscal_year  \n",
       "0        2009-12-01 07:45:00   6.95      13085.0  United Kingdom   2009-2010  \n",
       "1        2009-12-01 07:45:00   6.75      13085.0  United Kingdom   2009-2010  \n",
       "2        2009-12-01 07:45:00   6.75      13085.0  United Kingdom   2009-2010  \n",
       "3        2009-12-01 07:45:00   2.10      13085.0  United Kingdom   2009-2010  \n",
       "4        2009-12-01 07:45:00   1.25      13085.0  United Kingdom   2009-2010  \n",
       "...                      ...    ...          ...             ...         ...  \n",
       "1067366  2011-12-09 12:50:00   2.10      12680.0          France   2010-2011  \n",
       "1067367  2011-12-09 12:50:00   4.15      12680.0          France   2010-2011  \n",
       "1067368  2011-12-09 12:50:00   4.15      12680.0          France   2010-2011  \n",
       "1067369  2011-12-09 12:50:00   4.95      12680.0          France   2010-2011  \n",
       "1067370  2011-12-09 12:50:00  18.00      12680.0          France   2010-2011  \n",
       "\n",
       "[1067371 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/home/jovyan/work/online_retail_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32ddeae3-2bc7-452d-9bff-d1ed2a0283f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND ‚Üí /home/jovyan/work/online_retail_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"/\", topdown=True):\n",
    "    if \"online_retail_combined.csv\" in files:\n",
    "        print(\"FOUND ‚Üí\", os.path.join(root, \"online_retail_combined.csv\"))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3f88f6-40f0-488f-93ae-4e36dd518dac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Retail ETL Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ef83c5-9da1-4ead-afc7-7baf3a148e6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://42ba6cbfebdf:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Retail ETL Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x77d54035f350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe647f63-3571-4cdb-9b5e-76df21ddd915",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('online_retail_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f5547e9-fee2-471b-a574-0bc169fc869e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "|    _c0|      _c1|                 _c2|     _c3|                _c4|  _c5|        _c6|           _c7|        _c8|\n",
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "|Invoice|StockCode|         Description|Quantity|        InvoiceDate|Price|Customer ID|       Country|fiscal_year|\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|2009-12-01 07:45:00| 6.95|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|2009-12-01 07:45:00| 6.75|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|2009-12-01 07:45:00| 6.75|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|2009-12-01 07:45:00|  2.1|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|2009-12-01 07:45:00| 1.25|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|2009-12-01 07:45:00| 1.65|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|2009-12-01 07:45:00| 1.25|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|2009-12-01 07:45:00| 5.95|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489435|    22350|           CAT BOWL |      12|2009-12-01 07:46:00| 2.55|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|2009-12-01 07:46:00| 3.75|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489435|    22195|HEART MEASURING S...|      24|2009-12-01 07:46:00| 1.65|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489435|    22353|LUNCHBOX WITH CUT...|      12|2009-12-01 07:46:00| 2.55|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489436|   48173C|DOOR MAT BLACK FL...|      10|2009-12-01 09:06:00| 5.95|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    21755|LOVE BUILDING BLO...|      18|2009-12-01 09:06:00| 5.45|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    21754|HOME BUILDING BLO...|       3|2009-12-01 09:06:00| 5.95|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    84879|ASSORTED COLOUR B...|      16|2009-12-01 09:06:00| 1.69|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    22119| PEACE WOODEN BLO...|       3|2009-12-01 09:06:00| 6.95|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    22142|CHRISTMAS CRAFT W...|      12|2009-12-01 09:06:00| 1.45|    13078.0|United Kingdom|  2009-2010|\n",
      "| 489436|    22296|HEART IVORY TRELL...|      12|2009-12-01 09:06:00| 1.65|    13078.0|United Kingdom|  2009-2010|\n",
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e1d51b6-c390-4d55-8ed3-a57db3d11477",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('online_retail_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97436abc-cbca-46ff-a4b7-3bbc6c295ea1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab5f5c96-1474-4cde-8a19-b0d3f2e09db8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- fiscal_year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57a731d8-5735-4200-a009-734755e6a28d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Invoice='489434', StockCode='85048', Description='15CM CHRISTMAS GLASS BALL 20 LIGHTS', Quantity='12', InvoiceDate='2009-12-01 07:45:00', Price='6.95', Customer ID='13085.0', Country='United Kingdom', fiscal_year='2009-2010'),\n",
       " Row(Invoice='489434', StockCode='79323P', Description='PINK CHERRY LIGHTS', Quantity='12', InvoiceDate='2009-12-01 07:45:00', Price='6.75', Customer ID='13085.0', Country='United Kingdom', fiscal_year='2009-2010')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d18f93-0402-4574-a7b9-ad623e58405e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://42ba6cbfebdf:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"RetailAnalysis\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.ui.port\", \"8084\")\n",
    "    .config(\"spark.driver.host\", \"jupyter\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f\"Spark UI is available at: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "827ecc80-9a77-4540-803c-d48ea3c17d6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(0, 1_000_000)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7e44519-5caf-4e12-99a2-5b6988fa2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da786675-567b-4da2-a602-c830ef9cbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING RETAIL ETL PIPELINE\n",
      "======================================================================\n",
      "\n",
      "[1/9] Creating Spark Session...\n",
      "‚úÖ Spark Session Created!\n",
      "\n",
      "======================================================================\n",
      "üåê SPARK UI INFORMATION\n",
      "======================================================================\n",
      "Spark UI URL: http://jupyter:4040\n",
      "Application ID: app-20251221152118-0007\n",
      "Application Name: Retail ETL Pipeline\n",
      "Master: spark://spark-master:7077\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ STARTING RETAIL ETL PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Create Spark Session\n",
    "print(\"\\n[1/9] Creating Spark Session...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail ETL Pipeline\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"7078\") \\\n",
    "    .config(\"spark.blockManager.port\", \"7079\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.cores.max\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created!\")\n",
    "\n",
    "# Verify connection\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üåê SPARK UI INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78e67c5f-14a5-4dd6-89cf-47beb912b865",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Country</th>\n",
       "      <th>fiscal_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13085.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2009-2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067366</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067367</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067368</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067369</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067370</th>\n",
       "      <td>581587</td>\n",
       "      <td>POST</td>\n",
       "      <td>POSTAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "      <td>2010-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1067371 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Invoice StockCode                          Description  Quantity  \\\n",
       "0        489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1        489434    79323P                   PINK CHERRY LIGHTS        12   \n",
       "2        489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3        489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4        489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "...         ...       ...                                  ...       ...   \n",
       "1067366  581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "1067367  581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "1067368  581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "1067369  581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "1067370  581587      POST                              POSTAGE         1   \n",
       "\n",
       "                 InvoiceDate  Price  Customer ID         Country fiscal_year  \n",
       "0        2009-12-01 07:45:00   6.95      13085.0  United Kingdom   2009-2010  \n",
       "1        2009-12-01 07:45:00   6.75      13085.0  United Kingdom   2009-2010  \n",
       "2        2009-12-01 07:45:00   6.75      13085.0  United Kingdom   2009-2010  \n",
       "3        2009-12-01 07:45:00   2.10      13085.0  United Kingdom   2009-2010  \n",
       "4        2009-12-01 07:45:00   1.25      13085.0  United Kingdom   2009-2010  \n",
       "...                      ...    ...          ...             ...         ...  \n",
       "1067366  2011-12-09 12:50:00   2.10      12680.0          France   2010-2011  \n",
       "1067367  2011-12-09 12:50:00   4.15      12680.0          France   2010-2011  \n",
       "1067368  2011-12-09 12:50:00   4.15      12680.0          France   2010-2011  \n",
       "1067369  2011-12-09 12:50:00   4.95      12680.0          France   2010-2011  \n",
       "1067370  2011-12-09 12:50:00  18.00      12680.0          France   2010-2011  \n",
       "\n",
       "[1067371 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/home/jovyan/work/online_retail_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a17bb745-3e02-41d6-84fe-2ddf6da9a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Loading raw data...\n",
      "‚úÖ Loaded 1,067,371 rows\n",
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Customer ID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- fiscal_year: string (nullable = true)\n",
      "\n",
      "\n",
      "[2/8] Running data quality checks...\n",
      "\n",
      "üìä Null value counts:\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|fiscal_year|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|      0|        0|       4382|       0|          0|    0|     243007|      0|          0|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "\n",
      "\n",
      "üìã Total rows: 1,067,371\n",
      "üìã Distinct rows: 1,055,238\n",
      "üìã Duplicates: 12,133\n",
      "\n",
      "[3/8] Cleaning data...\n",
      "‚úÖ After cleaning: 793,609 rows\n",
      "   Removed: 273,762 rows\n",
      "\n",
      "[4/8] Transforming data...\n",
      "‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\n",
      "\n",
      "[5/8] Creating business metrics...\n",
      "‚úÖ Customer metrics created\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|CustomerID|TotalOrders|        TotalSpent|     AvgOrderValue|UniqueInvoices|  FirstPurchaseDate|   LastPurchaseDate|DaysSinceFirstPurchase|\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|   13178.0|        687|14059.189999999999| 20.46461426491994|            21|2009-12-16 14:00:00|2011-11-13 16:14:00|                   697|\n",
      "|   15039.0|       2749|          39726.89|14.451396871589669|            93|2009-12-10 11:39:00|2011-11-30 11:31:00|                   720|\n",
      "|   16939.0|         41|            433.85|10.581707317073171|             3|2010-02-28 13:33:00|2010-08-08 12:24:00|                   161|\n",
      "|   17966.0|        113|1963.9199999999998|17.379823008849556|            10|2010-01-18 13:24:00|2011-11-02 10:26:00|                   653|\n",
      "|   17955.0|         22|             949.9| 43.17727272727273|             6|2010-02-12 16:27:00|2011-05-25 12:00:00|                   467|\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ Product metrics created\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "|StockCode|         Description|TotalQuantitySold|      TotalRevenue|UniqueCustomers|          AvgPrice|\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "|    22423|REGENCY CAKESTAND...|            24858| 285992.3500000001|           1314|12.459903169014098|\n",
      "|   85123A|WHITE HANGING HEA...|            93520| 251731.2600000002|           1490|2.8695120046847347|\n",
      "|    23843|PAPER CRAFT , LIT...|            80995|          168469.6|              1|              2.08|\n",
      "|        M|              Manual|             9501|151951.91999999993|            438|209.37911301859788|\n",
      "|   85099B|JUMBO BAG RED RET...|            75597|136684.79000000036|            860|1.9725427509293596|\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "[6/8] Feature engineering...\n",
      "‚úÖ RFM analysis complete\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "|CustomerID|Recency|Frequency|          Monetary|R_Score|F_Score|M_Score|RFM_Score|\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "|   13178.0|     26|       21|14059.189999999999|      4|      5|      5|      455|\n",
      "|   15039.0|      9|       93| 39726.89000000002|      5|      5|      5|      555|\n",
      "|   16939.0|    488|        3|            433.85|      1|      3|      2|      132|\n",
      "|   17966.0|     37|       10|           1963.92|      4|      5|      4|      454|\n",
      "|   17955.0|    198|        6|             949.9|      2|      4|      3|      243|\n",
      "|   12891.0|    185|       11|             840.5|      3|      5|      3|      353|\n",
      "|   16553.0|    163|       33|16584.010000000002|      3|      5|      5|      355|\n",
      "|   14542.0|    185|        1|            103.25|      3|      2|      1|      321|\n",
      "|   12777.0|    457|        1|            519.45|      1|      2|      2|      122|\n",
      "|   12535.0|     91|        2|            716.35|      3|      3|      3|      333|\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[7/8] Detecting outliers...\n",
      "‚úÖ Found 64,720 outlier transactions\n",
      "   Lower bound: $-17.32\n",
      "   Upper bound: $42.07\n",
      "\n",
      "[8/8] Saving to Iceberg Lakehouse...\n",
      "‚ö†Ô∏è  Iceberg save error: An error occurred while calling o1191.createOrReplace.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 4 times, most recent failure: Lost task 0.3 in stage 89.0 (TID 78) (172.19.0.8 executor 0): org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3://lakehouse/warehouse/bronze/retail_transactions/data/Year=2010/Month=7/00000-78-908fb5a3-3429-4519-a7ad-ea70e296b347-0-00001.parquet\n",
      "\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n",
      "\tat org.apache.iceberg.hadoop.HadoopOutputFile.fromPath(HadoopOutputFile.java:53)\n",
      "\tat org.apache.iceberg.hadoop.HadoopFileIO.newOutputFile(HadoopFileIO.java:97)\n",
      "\tat org.apache.iceberg.io.ResolvingFileIO.newOutputFile(ResolvingFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.OutputFileFactory.newOutputFile(OutputFileFactory.java:117)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.newFile(RollingFileWriter.java:115)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:106)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n",
      "\t... 31 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:577)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.iceberg.exceptions.RuntimeIOException: Failed to get file system for path: s3://lakehouse/warehouse/bronze/retail_transactions/data/Year=2010/Month=7/00000-78-908fb5a3-3429-4519-a7ad-ea70e296b347-0-00001.parquet\n",
      "\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:58)\n",
      "\tat org.apache.iceberg.hadoop.HadoopOutputFile.fromPath(HadoopOutputFile.java:53)\n",
      "\tat org.apache.iceberg.hadoop.HadoopFileIO.newOutputFile(HadoopFileIO.java:97)\n",
      "\tat org.apache.iceberg.io.ResolvingFileIO.newOutputFile(ResolvingFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.OutputFileFactory.newOutputFile(OutputFileFactory.java:117)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.newFile(RollingFileWriter.java:115)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:106)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.<init>(RollingDataWriter.java:47)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.newWriter(FanoutDataWriter.java:53)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.writer(FanoutWriter.java:63)\n",
      "\tat org.apache.iceberg.io.FanoutWriter.write(FanoutWriter.java:51)\n",
      "\tat org.apache.iceberg.io.FanoutDataWriter.write(FanoutDataWriter.java:31)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:781)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$PartitionedDataWriter.write(SparkWrite.java:751)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n",
      "\t... 31 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 2: Load the data using Spark\n",
    "print(\"\\n[1/8] Loading raw data...\")\n",
    "df_raw = spark.read.csv(\"/opt/spark-data/online_retail_combined.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "row_count = df_raw.count()\n",
    "print(f\"‚úÖ Loaded {row_count:,} rows\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "\n",
    "# STEP 1: DATA QUALITY CHECKS\n",
    "# ============================================================\n",
    "print(\"\\n[2/8] Running data quality checks...\")\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "print(\"\\nüìä Null value counts:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Check for duplicates\n",
    "total_rows = df_raw.count()\n",
    "distinct_rows = df_raw.distinct().count()\n",
    "print(f\"\\nüìã Total rows: {total_rows:,}\")\n",
    "print(f\"üìã Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"üìã Duplicates: {total_rows - distinct_rows:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: DATA CLEANING\n",
    "# ============================================================\n",
    "print(\"\\n[3/8] Cleaning data...\")\n",
    "\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"Invoice\").isNotNull()) \\\n",
    "    .filter(col(\"StockCode\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"Price\") > 0) \\\n",
    "    .filter(col(\"Customer ID\").isNotNull()) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "print(f\"‚úÖ After cleaning: {df_cleaned.count():,} rows\")\n",
    "print(f\"   Removed: {df_raw.count() - df_cleaned.count():,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: DATA TRANSFORMATION\n",
    "# ============================================================\n",
    "print(\"\\n[4/8] Transforming data...\")\n",
    "\n",
    "df_transformed = df_cleaned \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"Customer ID\").cast(\"string\")) \\\n",
    "    .drop(\"Customer ID\")\n",
    "\n",
    "print(\"‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: CREATE BUSINESS METRICS\n",
    "# ============================================================\n",
    "print(\"\\n[5/8] Creating business metrics...\")\n",
    "\n",
    "# Customer-level aggregations\n",
    "df_customer_metrics = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        count(\"Invoice\").alias(\"TotalOrders\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalSpent\"),\n",
    "        avg(\"TotalAmount\").alias(\"AvgOrderValue\"),\n",
    "        countDistinct(\"Invoice\").alias(\"UniqueInvoices\"),\n",
    "        min(\"InvoiceDate\").alias(\"FirstPurchaseDate\"),\n",
    "        max(\"InvoiceDate\").alias(\"LastPurchaseDate\")\n",
    "    ) \\\n",
    "    .withColumn(\"DaysSinceFirstPurchase\", \n",
    "                datediff(col(\"LastPurchaseDate\"), col(\"FirstPurchaseDate\")))\n",
    "\n",
    "print(\"‚úÖ Customer metrics created\")\n",
    "df_customer_metrics.show(5)\n",
    "\n",
    "# Product-level aggregations\n",
    "df_product_metrics = df_transformed.groupBy(\"StockCode\", \"Description\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalRevenue\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"UniqueCustomers\"),\n",
    "        avg(\"Price\").alias(\"AvgPrice\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"TotalRevenue\"))\n",
    "\n",
    "print(\"\\n‚úÖ Product metrics created\")\n",
    "df_product_metrics.show(5)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "print(\"\\n[6/8] Feature engineering...\")\n",
    "\n",
    "# RFM Analysis (Recency, Frequency, Monetary)\n",
    "from datetime import datetime\n",
    "max_date = df_transformed.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "df_rfm = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Invoice\").alias(\"Frequency\"),\n",
    "        sum(\"TotalAmount\").alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "# Add RFM scores (1-5 scale)\n",
    "rfm_quantiles = df_rfm.approxQuantile([\"Recency\", \"Frequency\", \"Monetary\"], \n",
    "                                       [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "\n",
    "df_rfm_scored = df_rfm \\\n",
    "    .withColumn(\"R_Score\", \n",
    "                when(col(\"Recency\") <= rfm_quantiles[0][0], 5)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][1], 4)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][2], 3)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][3], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"F_Score\",\n",
    "                when(col(\"Frequency\") >= rfm_quantiles[1][3], 5)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][2], 4)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][1], 3)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"M_Score\",\n",
    "                when(col(\"Monetary\") >= rfm_quantiles[2][3], 5)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][2], 4)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][1], 3)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"RFM_Score\", \n",
    "                concat(col(\"R_Score\"), col(\"F_Score\"), col(\"M_Score\")))\n",
    "\n",
    "print(\"‚úÖ RFM analysis complete\")\n",
    "df_rfm_scored.show(10)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: OUTLIER DETECTION\n",
    "# ============================================================\n",
    "print(\"\\n[7/8] Detecting outliers...\")\n",
    "\n",
    "# Find outliers using IQR method\n",
    "quantiles = df_transformed.approxQuantile(\"TotalAmount\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_outliers = df_transformed.filter(\n",
    "    (col(\"TotalAmount\") < lower_bound) | \n",
    "    (col(\"TotalAmount\") > upper_bound)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Found {df_outliers.count():,} outlier transactions\")\n",
    "print(f\"   Lower bound: ${lower_bound:.2f}\")\n",
    "print(f\"   Upper bound: ${upper_bound:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# STEP 7: SAVE TO ICEBERG LAKEHOUSE (YOUR CATALOG!)\n",
    "# ============================================================\n",
    "print(\"\\n[8/8] Saving to Iceberg Lakehouse...\")\n",
    "\n",
    "try:\n",
    "    # BRONZE: Raw cleaned data\n",
    "    df_transformed.writeTo(\"demo.bronze.retail_transactions\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .partitionedBy(\"Year\", \"Month\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ BRONZE: demo.bronze.retail_transactions\")\n",
    "\n",
    "    # SILVER: Customer metrics  \n",
    "    df_customer_metrics.writeTo(\"demo.silver.customer_metrics\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.customer_metrics\")\n",
    "\n",
    "    # SILVER: Product metrics\n",
    "    df_product_metrics.writeTo(\"demo.silver.product_metrics\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.product_metrics\")\n",
    "\n",
    "    # SILVER: RFM analysis\n",
    "    df_rfm_scored.writeTo(\"demo.silver.rfm_analysis\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.rfm_analysis\")\n",
    "\n",
    "    # GOLD: Top customers by RFM score\n",
    "    df_top_customers = df_rfm_scored.filter(col(\"RFM_Score\") >= \"555\") \\\n",
    "        .select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\")\n",
    "    \n",
    "    df_top_customers.writeTo(\"demo.gold.top_customers\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ GOLD: demo.gold.top_customers\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Iceberg save error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d8bc12-e841-404d-8b05-3a945098a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "|Invoice|StockCode|         Description|Quantity|        InvoiceDate|Price|Customer ID|       Country|fiscal_year|\n",
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|2009-12-01 07:45:00| 6.95|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|2009-12-01 07:45:00| 6.75|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|2009-12-01 07:45:00| 6.75|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|2009-12-01 07:45:00|  2.1|    13085.0|United Kingdom|  2009-2010|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|2009-12-01 07:45:00| 1.25|    13085.0|United Kingdom|  2009-2010|\n",
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv(\"/opt/spark-data/online_retail_combined.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "df_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee6e7a9-6f15-41b6-9688-7dd005fe7774",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Minio\n",
      "  Downloading minio-7.2.20-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from Minio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from Minio) (2023.7.22)\n",
      "Collecting pycryptodome (from Minio)\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from Minio) (4.8.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from Minio) (2.0.7)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->Minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->Minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->Minio) (2.21)\n",
      "Downloading minio-7.2.20-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m731.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycryptodome, Minio\n",
      "Successfully installed Minio-7.2.20 pycryptodome-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e87c95-dfe4-41cc-895e-e7373c321705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully!\n",
      "Spark version: 3.5.0\n",
      "App name: Retail ETL Pipeline\n",
      "\n",
      "üîç Testing MinIO connection...\n",
      "Available catalogs:\n",
      "  - spark_catalog\n",
      "\n",
      "üéØ Spark is ready for ETL!\n",
      "Next steps:\n",
      "  1. Create schema: spark.sql('CREATE SCHEMA IF NOT EXISTS iceberg.retail')\n",
      "  2. Load data from MinIO\n",
      "  3. Transform and write to Iceberg tables\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from minio import Minio\n",
    "import io\n",
    "\n",
    "# Create Spark Session with Iceberg + MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail ETL Pipeline\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\",\n",
    "            \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\",\n",
    "            \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\",\n",
    "            \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\",\n",
    "            \"s3a://lakehouse/warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.io-impl\",\n",
    "            \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\n",
    "            \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "        ])\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"App name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Test connection to MinIO\n",
    "print(\"\\nüîç Testing MinIO connection...\")\n",
    "try:\n",
    "    # List catalogs\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "    print(\"Available catalogs:\")\n",
    "    for catalog in catalogs:\n",
    "        print(f\"  - {catalog[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Note: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Spark is ready for ETL!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Create schema: spark.sql('CREATE SCHEMA IF NOT EXISTS iceberg.retail')\")\n",
    "print(\"  2. Load data from MinIO\")\n",
    "print(\"  3. Transform and write to Iceberg tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e22f57-c254-4e80-be1a-0c67ef5358ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/10] Creating Iceberg schemas...\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# STEP 2: CREATE ICEBERG SCHEMA (NAMESPACE)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/10] Creating Iceberg schemas...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE NAMESPACE IF NOT EXISTS iceberg.bronze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE NAMESPACE IF NOT EXISTS iceberg.silver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE NAMESPACE IF NOT EXISTS iceberg.gold\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]"
     ]
    }
   ],
   "source": [
    "# STEP 2: CREATE ICEBERG SCHEMA (NAMESPACE)\n",
    "# ============================================================\n",
    "print(\"\\n[2/10] Creating Iceberg schemas...\")\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.gold\")\n",
    "\n",
    "print(\"‚úÖ Created namespaces: bronze, silver, gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e0b4d8-eff4-4e6e-baf0-a0aee176609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sql\n",
      "  Downloading sql-2022.4.0.tar.gz (4.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sql\n",
      "  Building wheel for sql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sql: filename=sql-2022.4.0-py3-none-any.whl size=4306 sha256=ddceb7ed61c2c130850dbdab40db8cd8cbe27bc0483007f58fdc8d1d95aab477\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a6/f1/62/be6faba20c8384c5766c0332c93841b610ab2602d6ae312bbf\n",
      "Successfully built sql\n",
      "Installing collected packages: sql\n",
      "Successfully installed sql-2022.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfe32708-85e5-4363-8378-397a97eecd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/10] Creating Spark Session with Local JARs...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1: CREATE SPARK SESSION (FIXED)\n",
    "# ============================================================\n",
    "print(\"\\n[1/10] Creating Spark Session with Local JARs...\")\n",
    "\n",
    "# Inside the container, ./shared is mapped to /opt/shared-data\n",
    "jar_path = \"/opt/shared-data\"\n",
    "iceberg_runtime = f\"{jar_path}/iceberg-spark-runtime-3.5_2.12-1.5.0.jar\"\n",
    "aws_bundle = f\"{jar_path}/iceberg-aws-bundle-1.5.0.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail ETL Pipeline - Iceberg REST\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", f\"{iceberg_runtime},{aws_bundle}\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{iceberg_runtime}:{aws_bundle}\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{iceberg_runtime}:{aws_bundle}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://lakehouse/warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"minio\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"minio123\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.client.region\", \"us-east-1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bab20e50-ae18-425f-aba3-55363efae89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Worker/Driver synchronization successful!\n"
     ]
    }
   ],
   "source": [
    "# Check if the session can actually see the S3 classes now\n",
    "try:\n",
    "    spark._jvm.software.amazon.awssdk.services.s3.model.S3Exception\n",
    "    print(\"‚úÖ Worker/Driver synchronization successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Still missing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8f405f1-9296-4b8d-b243-65f9cc31fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/10] Creating Iceberg namespaces...\n",
      "‚ö†Ô∏è  bronze: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n",
      "\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]\n",
      "‚ö†Ô∏è  silver: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n",
      "\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]\n",
      "‚ö†Ô∏è  gold: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n",
      "\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]\n",
      "\n",
      "Available namespaces in Iceberg:\n",
      "‚ö†Ô∏è  Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n",
      "\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: CREATE ICEBERG NAMESPACES\n",
    "# ============================================================\n",
    "print(\"\\n[2/10] Creating Iceberg namespaces...\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.bronze\")\n",
    "    print(\"‚úÖ Created: iceberg.bronze\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  bronze: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.silver\")\n",
    "    print(\"‚úÖ Created: iceberg.silver\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  silver: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.gold\")\n",
    "    print(\"‚úÖ Created: iceberg.gold\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  gold: {e}\")\n",
    "\n",
    "# List namespaces\n",
    "print(\"\\nAvailable namespaces in Iceberg:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fad9f4d-79b5-4998-a350-3127947025a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/10] Loading raw data...\n",
      "‚úÖ Loaded 1,067,371 rows\n",
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Customer ID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- fiscal_year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: LOAD RAW DATA\n",
    "# ============================================================\n",
    "print(\"\\n[3/10] Loading raw data...\")\n",
    "df_raw = spark.read.csv(\"/opt/spark-data/online_retail_combined.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "row_count = df_raw.count()\n",
    "print(f\"‚úÖ Loaded {row_count:,} rows\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8b1b72-7183-475b-adc9-fc6151e7bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/10] Running data quality checks...\n",
      "\n",
      "üìä Null value counts:\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|fiscal_year|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|      0|        0|       4382|       0|          0|    0|     243007|      0|          0|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "\n",
      "\n",
      "üìã Total rows: 1,067,371\n",
      "üìã Distinct rows: 1,055,238\n",
      "üìã Duplicates: 12,133\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: DATA QUALITY CHECKS\n",
    "# ============================================================\n",
    "print(\"\\n[4/10] Running data quality checks...\")\n",
    "\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "print(\"\\nüìä Null value counts:\")\n",
    "null_counts.show()\n",
    "\n",
    "total_rows = df_raw.count()\n",
    "distinct_rows = df_raw.distinct().count()\n",
    "print(f\"\\nüìã Total rows: {total_rows:,}\")\n",
    "print(f\"üìã Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"üìã Duplicates: {total_rows - distinct_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e3f8c3-7f24-40e5-b2f0-37cdfd92d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/10] Cleaning data...\n",
      "‚úÖ After cleaning: 793,609 rows\n",
      "   Removed: 273,762 rows\n",
      "\n",
      "[6/10] Transforming data...\n",
      "‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: DATA CLEANING\n",
    "# ============================================================\n",
    "print(\"\\n[5/10] Cleaning data...\")\n",
    "\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"Invoice\").isNotNull()) \\\n",
    "    .filter(col(\"StockCode\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"Price\") > 0) \\\n",
    "    .filter(col(\"Customer ID\").isNotNull()) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "print(f\"‚úÖ After cleaning: {df_cleaned.count():,} rows\")\n",
    "print(f\"   Removed: {df_raw.count() - df_cleaned.count():,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: DATA TRANSFORMATION\n",
    "# ============================================================\n",
    "print(\"\\n[6/10] Transforming data...\")\n",
    "\n",
    "df_transformed = df_cleaned \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"Customer ID\").cast(\"string\")) \\\n",
    "    .drop(\"Customer ID\")\n",
    "\n",
    "print(\"‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d95a101-1422-4540-88f2-c8f1631d271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/10] Setting Iceberg as default catalog and creating namespaces...\n",
      "Current catalog: spark_catalog\n",
      "‚ö†Ô∏è  Error setting catalog: \n",
      "[PARSE_SYNTAX_ERROR] Syntax error at or near 'iceberg': extra input 'iceberg'.(line 1, pos 12)\n",
      "\n",
      "== SQL ==\n",
      "USE CATALOG iceberg\n",
      "------------^^^\n",
      "\n",
      "\n",
      "Creating namespaces...\n",
      "‚úÖ Created: bronze\n",
      "‚úÖ Created: silver\n",
      "‚úÖ Created: gold\n",
      "\n",
      "Available namespaces in Iceberg:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1891/3170376295.py\", line 15, in <module>\n",
      "    spark.sql(\"USE CATALOG iceberg\")\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 1631, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.ParseException: \n",
      "[PARSE_SYNTAX_ERROR] Syntax error at or near 'iceberg': extra input 'iceberg'.(line 1, pos 12)\n",
      "\n",
      "== SQL ==\n",
      "USE CATALOG iceberg\n",
      "------------^^^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2: SET DEFAULT CATALOG & CREATE ICEBERG NAMESPACES\n",
    "# ============================================================\n",
    "print(\"\\n[2/10] Setting Iceberg as default catalog and creating namespaces...\")\n",
    "\n",
    "# Check current catalog\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    print(f\"Current catalog: {current_catalog}\")\n",
    "except:\n",
    "    print(\"Could not determine current catalog\")\n",
    "\n",
    "# Set iceberg as the current catalog\n",
    "try:\n",
    "    spark.sql(\"USE CATALOG iceberg\")\n",
    "    print(\"‚úÖ Set current catalog to: iceberg\")\n",
    "    \n",
    "    # Verify\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    print(f\"Verified current catalog: {current_catalog}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error setting catalog: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Create namespaces\n",
    "print(\"\\nCreating namespaces...\")\n",
    "namespaces = [\"bronze\", \"silver\", \"gold\"]\n",
    "for ns in namespaces:\n",
    "    try:\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {ns}\")\n",
    "        print(f\"‚úÖ Created: {ns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {ns}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# List namespaces\n",
    "print(\"\\nAvailable namespaces in Iceberg:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7d8419-4229-4da4-a343-733702a3be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully!\n",
      "Spark Version: 3.5.0\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create Spark session with Polaris catalog\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Lakehouse-Polaris\")\n",
    "    \n",
    "    # Connect to Spark cluster\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.driver.host\", \"jupyter\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    \n",
    "    # Iceberg Extensions (REQUIRED)\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    \n",
    "    # Polaris REST Catalog\n",
    "    .config(\"spark.sql.catalog.polaris\", \n",
    "            \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.polaris.catalog-impl\", \n",
    "            \"org.apache.iceberg.rest.RESTCatalog\")\n",
    "    .config(\"spark.sql.catalog.polaris.uri\", \n",
    "            \"http://polaris:8181\")\n",
    "    .config(\"spark.sql.catalog.polaris.credential\", \n",
    "            \"8bc42068486af1a8:d3eed0a93918a2fa99850097d43b8fd0\")  # ‚Üê NEW CREDENTIAL\n",
    "    .config(\"spark.sql.catalog.polaris.warehouse\", \n",
    "            \"lakehouse\")\n",
    "    \n",
    "    # S3/MinIO for data storage\n",
    "    .config(\"spark.sql.catalog.polaris.io-impl\", \n",
    "            \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.endpoint\", \n",
    "            \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.access-key-id\", \n",
    "            \"minio\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.secret-access-key\", \n",
    "            \"minio123\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.path-style-access\", \n",
    "            \"true\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303927ba-41b1-4f6f-97b0-052d631e02b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available catalogs:\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Show available catalogs\n",
    "print(\"üìã Available catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cb8426-10ad-4279-852c-d22d00b9a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Spark configs related to Iceberg/Polaris:\n",
      "spark.sql.catalog.polaris.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.polaris.s3.path-style-access = true\n",
      "spark.sql.catalog.polaris.warehouse = lakehouse\n",
      "spark.sql.catalog.polaris.catalog-impl = org.apache.iceberg.rest.RESTCatalog\n",
      "spark.sql.catalog.polaris = org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.catalog.polaris.s3.access-key-id = minio\n",
      "spark.sql.catalog.polaris.s3.secret-access-key = minio123\n",
      "spark.sql.catalog.polaris.uri = http://polaris:8181\n",
      "spark.sql.catalog.polaris.credential = 8bc42068486af1a8:d3eed0a93918a2fa99850097d43b8fd0\n",
      "spark.sql.catalog.polaris.s3.endpoint = http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# Check Spark configuration\n",
    "print(\"Current Spark configs related to Iceberg/Polaris:\")\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    if 'iceberg' in conf[0].lower() or 'polaris' in conf[0].lower() or 'catalog' in conf[0].lower():\n",
    "        print(f\"{conf[0]} = {conf[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305fd3f6-a68e-4a94-9839-e7021f3ec2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking classpath for Iceberg JARs:\n",
      "\n",
      "==================================================\n",
      "Checking if Iceberg classes can be loaded:\n",
      "‚úÖ SparkCatalog class found!\n",
      "‚úÖ RESTCatalog class found!\n"
     ]
    }
   ],
   "source": [
    "# Check JARs (fixed version)\n",
    "import os\n",
    "\n",
    "# Method 1: Check classpath\n",
    "print(\"Checking classpath for Iceberg JARs:\")\n",
    "classpath = spark.sparkContext._jvm.System.getProperty(\"java.class.path\")\n",
    "for entry in classpath.split(\":\"):\n",
    "    if 'iceberg' in entry.lower():\n",
    "        print(f\"‚úÖ Found: {entry}\")\n",
    "\n",
    "# Method 2: Check if Iceberg classes are available\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Checking if Iceberg classes can be loaded:\")\n",
    "try:\n",
    "    spark.sparkContext._jvm.org.apache.iceberg.spark.SparkCatalog\n",
    "    print(\"‚úÖ SparkCatalog class found!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SparkCatalog class NOT found: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sparkContext._jvm.org.apache.iceberg.rest.RESTCatalog\n",
    "    print(\"‚úÖ RESTCatalog class found!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå RESTCatalog class NOT found: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba427f95-6c3a-4d29-bcdd-293de093b3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Saving to Iceberg Lakehouse...\n",
      "‚ö†Ô∏è  Iceberg save error: An error occurred while calling o1786.createOrReplace.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:833)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:522)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:518)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:361)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:577)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 7: SAVE TO ICEBERG LAKEHOUSE (YOUR CATALOG!)\n",
    "# ============================================================\n",
    "print(\"\\n[8/8] Saving to Iceberg Lakehouse...\")\n",
    "\n",
    "try:\n",
    "    # BRONZE: Raw cleaned data\n",
    "    df_transformed.writeTo(\"demo.bronze.retail_transactions\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .partitionedBy(\"Year\", \"Month\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ BRONZE: demo.bronze.retail_transactions\")\n",
    "\n",
    "    # SILVER: Customer metrics  \n",
    "    df_customer_metrics.writeTo(\"demo.silver.customer_metrics\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.customer_metrics\")\n",
    "\n",
    "    # SILVER: Product metrics\n",
    "    df_product_metrics.writeTo(\"demo.silver.product_metrics\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.product_metrics\")\n",
    "\n",
    "    # SILVER: RFM analysis\n",
    "    df_rfm_scored.writeTo(\"demo.silver.rfm_analysis\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ SILVER: demo.silver.rfm_analysis\")\n",
    "\n",
    "    # GOLD: Top customers by RFM score\n",
    "    df_top_customers = df_rfm_scored.filter(col(\"RFM_Score\") >= \"555\") \\\n",
    "        .select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\")\n",
    "    \n",
    "    df_top_customers.writeTo(\"demo.gold.top_customers\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .createOrReplace()\n",
    "    print(\"‚úÖ GOLD: demo.gold.top_customers\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Iceberg save error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df7ee5d-d613-486e-a523-3b773e8e0365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:106)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create Spark Session (ICEBERG + REST)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRetail-Lakehouse-Iceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ---- Spark cluster ----\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark://spark-master:7077\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.host\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjupyter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.bindAddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ---- Iceberg Spark Extensions (MANDATORY) ----\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================================================\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ICEBERG REST CATALOG (catalog name MUST be \"iceberg\")\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# =========================================================\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg.catalog-impl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.rest.RESTCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.iceberg.uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://rest:8181\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Spark started successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:500\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:589\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    586\u001b[0m             jsparkSession, options\n\u001b[1;32m    587\u001b[0m         )\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    592\u001b[0m         jsparkSession, options\n\u001b[1;32m    593\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:106)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Stop any existing Spark session\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create Spark Session (ICEBERG + REST)\n",
    "# ------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Lakehouse-Iceberg\")\n",
    "\n",
    "    # ---- Spark cluster ----\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.driver.host\", \"jupyter\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "    # ---- Iceberg Spark Extensions (MANDATORY) ----\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    # =========================================================\n",
    "    # ICEBERG REST CATALOG (catalog name MUST be \"iceberg\")\n",
    "    # =========================================================\n",
    "    .config(\n",
    "        \"spark.sql.catalog.iceberg\",\n",
    "        \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.iceberg.catalog-impl\",\n",
    "        \"org.apache.iceberg.rest.RESTCatalog\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.iceberg.uri\",\n",
    "        \"http://rest:8181\"\n",
    "    )\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark started successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Validate\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìã Available catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"\\nüìÇ Namespaces in iceberg catalog:\")\n",
    "spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e554e0-f587-421b-9b22-978b1a23beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a7eba14-c2f0-4e17-b3ea-55f14a2d82db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Loading raw data...\n",
      "‚úÖ Loaded 1,067,371 rows\n",
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Customer ID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- fiscal_year: string (nullable = true)\n",
      "\n",
      "\n",
      "[2/8] Running data quality checks...\n",
      "\n",
      "üìä Null value counts:\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|fiscal_year|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|      0|        0|       4382|       0|          0|    0|     243007|      0|          0|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "\n",
      "\n",
      "üìã Total rows: 1,067,371\n",
      "üìã Distinct rows: 1,055,238\n",
      "üìã Duplicates: 12,133\n",
      "\n",
      "[3/8] Cleaning data...\n",
      "‚úÖ After cleaning: 793,609 rows\n",
      "   Removed: 273,762 rows\n",
      "\n",
      "[4/8] Transforming data...\n",
      "‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\n",
      "\n",
      "[5/8] Creating business metrics...\n",
      "‚úÖ Customer metrics created\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|CustomerID|TotalOrders|        TotalSpent|     AvgOrderValue|UniqueInvoices|  FirstPurchaseDate|   LastPurchaseDate|DaysSinceFirstPurchase|\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|   13178.0|        687|14059.189999999999| 20.46461426491994|            21|2009-12-16 14:00:00|2011-11-13 16:14:00|                   697|\n",
      "|   15039.0|       2749|          39726.89|14.451396871589669|            93|2009-12-10 11:39:00|2011-11-30 11:31:00|                   720|\n",
      "|   16939.0|         41|            433.85|10.581707317073171|             3|2010-02-28 13:33:00|2010-08-08 12:24:00|                   161|\n",
      "|   17966.0|        113|1963.9199999999998|17.379823008849556|            10|2010-01-18 13:24:00|2011-11-02 10:26:00|                   653|\n",
      "|   17955.0|         22|             949.9| 43.17727272727273|             6|2010-02-12 16:27:00|2011-05-25 12:00:00|                   467|\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ Product metrics created\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "|StockCode|         Description|TotalQuantitySold|      TotalRevenue|UniqueCustomers|          AvgPrice|\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "|    22423|REGENCY CAKESTAND...|            24858| 285992.3500000001|           1314|12.459903169014098|\n",
      "|   85123A|WHITE HANGING HEA...|            93520| 251731.2600000002|           1490|2.8695120046847347|\n",
      "|    23843|PAPER CRAFT , LIT...|            80995|          168469.6|              1|              2.08|\n",
      "|        M|              Manual|             9501|151951.91999999993|            438|209.37911301859788|\n",
      "|   85099B|JUMBO BAG RED RET...|            75597|136684.79000000036|            860|1.9725427509293596|\n",
      "+---------+--------------------+-----------------+------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "[6/8] Feature engineering...\n",
      "‚úÖ RFM analysis complete\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "|CustomerID|Recency|Frequency|          Monetary|R_Score|F_Score|M_Score|RFM_Score|\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "|   13178.0|     26|       21|14059.189999999999|      4|      5|      5|      455|\n",
      "|   15039.0|      9|       93| 39726.89000000002|      5|      5|      5|      555|\n",
      "|   16939.0|    488|        3|            433.85|      1|      3|      2|      132|\n",
      "|   17966.0|     37|       10|           1963.92|      4|      5|      4|      454|\n",
      "|   17955.0|    198|        6|             949.9|      2|      4|      3|      243|\n",
      "|   12891.0|    185|       11|             840.5|      3|      5|      3|      353|\n",
      "|   16553.0|    163|       33|16584.010000000002|      3|      5|      5|      355|\n",
      "|   14542.0|    185|        1|            103.25|      3|      2|      1|      321|\n",
      "|   12777.0|    457|        1|            519.45|      1|      2|      2|      122|\n",
      "|   12535.0|     91|        2|            716.35|      3|      3|      3|      333|\n",
      "+----------+-------+---------+------------------+-------+-------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[7/8] Detecting outliers...\n",
      "‚úÖ Found 64,720 outlier transactions\n",
      "   Lower bound: $-17.32\n",
      "   Upper bound: $42.07\n",
      "\n",
      "[8/8] Saving to Iceberg Lakehouse...\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 186\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[8/8] Saving to Iceberg Lakehouse...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# BRONZE ‚Äì FACT TABLE\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    177\u001b[0m (\n\u001b[1;32m    178\u001b[0m     \u001b[43mdf_transformed\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg.bronze.retail_transactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionedBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat-version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite.format.default\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite.metadata.delete-after-commit.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite.metadata.previous-versions-max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ BRONZE table written (partition overwrite)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# SILVER ‚Äì CUSTOMER METRICS\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:2127\u001b[0m, in \u001b[0;36mDataFrameWriterV2.overwritePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverwritePartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;124;03m    Overwrite all partition for which the data frame contains at least one row with the contents\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;124;03m    of the data frame in the output table.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m    partitions dynamically depending on the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 2: Load the data using Spark\n",
    "print(\"\\n[1/8] Loading raw data...\")\n",
    "df_raw = spark.read.csv(\"/opt/spark-data/online_retail_combined.csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)\n",
    "\n",
    "row_count = df_raw.count()\n",
    "print(f\"‚úÖ Loaded {row_count:,} rows\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "\n",
    "# STEP 1: DATA QUALITY CHECKS\n",
    "# ============================================================\n",
    "print(\"\\n[2/8] Running data quality checks...\")\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "print(\"\\nüìä Null value counts:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Check for duplicates\n",
    "total_rows = df_raw.count()\n",
    "distinct_rows = df_raw.distinct().count()\n",
    "print(f\"\\nüìã Total rows: {total_rows:,}\")\n",
    "print(f\"üìã Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"üìã Duplicates: {total_rows - distinct_rows:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: DATA CLEANING\n",
    "# ============================================================\n",
    "print(\"\\n[3/8] Cleaning data...\")\n",
    "\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"Invoice\").isNotNull()) \\\n",
    "    .filter(col(\"StockCode\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"Price\") > 0) \\\n",
    "    .filter(col(\"Customer ID\").isNotNull()) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "print(f\"‚úÖ After cleaning: {df_cleaned.count():,} rows\")\n",
    "print(f\"   Removed: {df_raw.count() - df_cleaned.count():,} rows\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: DATA TRANSFORMATION\n",
    "# ============================================================\n",
    "print(\"\\n[4/8] Transforming data...\")\n",
    "\n",
    "df_transformed = df_cleaned \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"Customer ID\").cast(\"string\")) \\\n",
    "    .drop(\"Customer ID\")\n",
    "\n",
    "print(\"‚úÖ Added columns: TotalAmount, Year, Month, DayOfWeek, Hour\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: CREATE BUSINESS METRICS\n",
    "# ============================================================\n",
    "print(\"\\n[5/8] Creating business metrics...\")\n",
    "\n",
    "# Customer-level aggregations\n",
    "df_customer_metrics = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        count(\"Invoice\").alias(\"TotalOrders\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalSpent\"),\n",
    "        avg(\"TotalAmount\").alias(\"AvgOrderValue\"),\n",
    "        countDistinct(\"Invoice\").alias(\"UniqueInvoices\"),\n",
    "        min(\"InvoiceDate\").alias(\"FirstPurchaseDate\"),\n",
    "        max(\"InvoiceDate\").alias(\"LastPurchaseDate\")\n",
    "    ) \\\n",
    "    .withColumn(\"DaysSinceFirstPurchase\", \n",
    "                datediff(col(\"LastPurchaseDate\"), col(\"FirstPurchaseDate\")))\n",
    "\n",
    "print(\"‚úÖ Customer metrics created\")\n",
    "df_customer_metrics.show(5)\n",
    "\n",
    "# Product-level aggregations\n",
    "df_product_metrics = df_transformed.groupBy(\"StockCode\", \"Description\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalRevenue\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"UniqueCustomers\"),\n",
    "        avg(\"Price\").alias(\"AvgPrice\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"TotalRevenue\"))\n",
    "\n",
    "print(\"\\n‚úÖ Product metrics created\")\n",
    "df_product_metrics.show(5)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "print(\"\\n[6/8] Feature engineering...\")\n",
    "\n",
    "# RFM Analysis (Recency, Frequency, Monetary)\n",
    "from datetime import datetime\n",
    "max_date = df_transformed.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "df_rfm = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Invoice\").alias(\"Frequency\"),\n",
    "        sum(\"TotalAmount\").alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "# Add RFM scores (1-5 scale)\n",
    "rfm_quantiles = df_rfm.approxQuantile([\"Recency\", \"Frequency\", \"Monetary\"], \n",
    "                                       [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "\n",
    "df_rfm_scored = df_rfm \\\n",
    "    .withColumn(\"R_Score\", \n",
    "                when(col(\"Recency\") <= rfm_quantiles[0][0], 5)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][1], 4)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][2], 3)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][3], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"F_Score\",\n",
    "                when(col(\"Frequency\") >= rfm_quantiles[1][3], 5)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][2], 4)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][1], 3)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"M_Score\",\n",
    "                when(col(\"Monetary\") >= rfm_quantiles[2][3], 5)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][2], 4)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][1], 3)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"RFM_Score\", \n",
    "                concat(col(\"R_Score\"), col(\"F_Score\"), col(\"M_Score\")))\n",
    "\n",
    "print(\"‚úÖ RFM analysis complete\")\n",
    "df_rfm_scored.show(10)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: OUTLIER DETECTION\n",
    "# ============================================================\n",
    "print(\"\\n[7/8] Detecting outliers...\")\n",
    "\n",
    "# Find outliers using IQR method\n",
    "quantiles = df_transformed.approxQuantile(\"TotalAmount\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_outliers = df_transformed.filter(\n",
    "    (col(\"TotalAmount\") < lower_bound) | \n",
    "    (col(\"TotalAmount\") > upper_bound)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Found {df_outliers.count():,} outlier transactions\")\n",
    "print(f\"   Lower bound: ${lower_bound:.2f}\")\n",
    "print(f\"   Upper bound: ${upper_bound:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# STEP 7: SAVE TO ICEBERG LAKEHOUSE (YOUR CATALOG!)\n",
    "# ============================================================\n",
    "print(\"\\n[8/8] Saving to Iceberg Lakehouse...\")\n",
    "# -------------------------\n",
    "# BRONZE ‚Äì FACT TABLE\n",
    "# -------------------------\n",
    "(\n",
    "    df_transformed\n",
    "    .writeTo(\"iceberg.bronze.retail_transactions\")\n",
    "    .using(\"iceberg\")\n",
    "    .partitionedBy(\"Year\", \"Month\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .tableProperty(\"write.format.default\", \"parquet\")\n",
    "    .tableProperty(\"write.metadata.delete-after-commit.enabled\", \"true\")\n",
    "    .tableProperty(\"write.metadata.previous-versions-max\", \"5\")\n",
    "    .overwritePartitions()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ BRONZE table written (partition overwrite)\")\n",
    "\n",
    "# -------------------------\n",
    "# SILVER ‚Äì CUSTOMER METRICS\n",
    "# -------------------------\n",
    "(\n",
    "    df_customer_metrics\n",
    "    .writeTo(\"iceberg.silver.customer_metrics\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER customer_metrics written\")\n",
    "\n",
    "# -------------------------\n",
    "# SILVER ‚Äì PRODUCT METRICS\n",
    "# -------------------------\n",
    "(\n",
    "    df_product_metrics\n",
    "    .writeTo(\"iceberg.silver.product_metrics\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER product_metrics written\")\n",
    "\n",
    "# -------------------------\n",
    "# SILVER ‚Äì RFM ANALYSIS\n",
    "# -------------------------\n",
    "(\n",
    "    df_rfm_scored\n",
    "    .writeTo(\"iceberg.silver.rfm_analysis\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER rfm_analysis written\")\n",
    "\n",
    "# -------------------------\n",
    "# GOLD ‚Äì TOP CUSTOMERS\n",
    "# -------------------------\n",
    "df_top_customers = (\n",
    "    df_rfm_scored\n",
    "    .filter(col(\"RFM_Score\") >= \"555\")\n",
    "    .select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_top_customers\n",
    "    .writeTo(\"iceberg.gold.top_customers\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GOLD top_customers written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3355a0f9-857a-4964-bbbc-7abc8bc09783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|   bronze|retail_transactions|      false|\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  793609|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN demo.bronze\").show()\n",
    "spark.sql(\"SELECT COUNT(*) FROM demo.bronze.retail_transactions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5d1e8ec-21f5-48cb-90fe-b2e0b1d5a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a118fad-a5ef-4e69-a924-b3c929d2a8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW NAMESPACES IN iceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8449b09-be97-46ab-8a91-d7686881046d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# STEP 7: SAVE TO ICEBERG LAKEHOUSE (YOUR CATALOG!)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# SILVER ‚Äì CUSTOMER METRICS\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m      7\u001b[0m (\n\u001b[1;32m      8\u001b[0m     \u001b[43mdf_customer_metrics\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg.silver.customer_metrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableProperty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat-version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ SILVER customer_metrics written\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# SILVER ‚Äì PRODUCT METRICS\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:2100\u001b[0m, in \u001b[0;36mDataFrameWriterV2.createOrReplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    Create a new table or replace an existing table with the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m    If the table exists, its configuration and data will be replaced.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 7: SAVE TO ICEBERG LAKEHOUSE (YOUR CATALOG!)\n",
    "# ============================================================\n",
    "\n",
    "# SILVER ‚Äì CUSTOMER METRICS\n",
    "# -------------------------\n",
    "(\n",
    "    df_customer_metrics\n",
    "    .writeTo(\"iceberg.silver.customer_metrics\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER customer_metrics written\")\n",
    "\n",
    "# -------------------------\n",
    "# SILVER ‚Äì PRODUCT METRICS\n",
    "# -------------------------\n",
    "(\n",
    "    df_product_metrics\n",
    "    .writeTo(\"iceberg.silver.product_metrics\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER product_metrics written\")\n",
    "\n",
    "# -------------------------\n",
    "# SILVER ‚Äì RFM ANALYSIS\n",
    "# -------------------------\n",
    "(\n",
    "    df_rfm_scored\n",
    "    .writeTo(\"iceberg.silver.rfm_analysis\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SILVER rfm_analysis written\")\n",
    "\n",
    "# -------------------------\n",
    "# GOLD ‚Äì TOP CUSTOMERS\n",
    "# -------------------------\n",
    "df_top_customers = (\n",
    "    df_rfm_scored\n",
    "    .filter(col(\"RFM_Score\") >= \"555\")\n",
    "    .select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_top_customers\n",
    "    .writeTo(\"iceberg.gold.top_customers\")\n",
    "    .using(\"iceberg\")\n",
    "    .tableProperty(\"format-version\", \"2\")\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GOLD top_customers written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f32b46a-f46c-45de-8fed-bfc8d96f4832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mdf_transformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg.bronze.retail_transactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionedBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:2100\u001b[0m, in \u001b[0;36mDataFrameWriterV2.createOrReplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    Create a new table or replace an existing table with the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m    If the table exists, its configuration and data will be replaced.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot initialize FileIO implementation org.apache.iceberg.aws.s3.S3FileIO: Cannot find constructor for interface org.apache.iceberg.io.FileIO\n\tMissing org.apache.iceberg.aws.s3.S3FileIO [java.lang.NoClassDefFoundError: software/amazon/awssdk/services/s3/model/S3Exception]"
     ]
    }
   ],
   "source": [
    "df_transformed.writeTo(\n",
    "    \"iceberg.bronze.retail_transactions\"\n",
    ").using(\"iceberg\") \\\n",
    " .partitionedBy(\"Year\", \"Month\") \\\n",
    " .createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b972e81-ccfd-4675-88a2-c5379d32b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "635225f5-ac23-4d01-a59e-b0681f22d7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o276.sql.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'iceberg': org.apache.iceberg.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.ShowNamespaces.mapChildren(v2Commands.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 78 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW NAMESPACES IN iceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o276.sql.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'iceberg': org.apache.iceberg.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1925)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.ShowNamespaces.mapChildren(v2Commands.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 78 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0da92f-ce70-4fb9-9ba9-343106ae7d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3d8c57-1e49-44ea-9f4d-5f6c466b03b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN iceberg.silver\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8505c82d-aba8-4e7b-8c8e-ba9df60be2e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.sql.\n: org.apache.iceberg.exceptions.ServiceFailureException: Server error: SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:201)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:313)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:252)\n\tat org.apache.iceberg.rest.HTTPClient.post(HTTPClient.java:358)\n\tat org.apache.iceberg.rest.RESTClient.post(RESTClient.java:112)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$Builder.create(RESTSessionCatalog.java:673)\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:247)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:200)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mCREATE TABLE iceberg.silver._debug_test (\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m  id INT,\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m  name STRING\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43mUSING iceberg\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.sql.\n: org.apache.iceberg.exceptions.ServiceFailureException: Server error: SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:201)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:313)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:252)\n\tat org.apache.iceberg.rest.HTTPClient.post(HTTPClient.java:358)\n\tat org.apache.iceberg.rest.RESTClient.post(RESTClient.java:112)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$Builder.create(RESTSessionCatalog.java:673)\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:247)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:200)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE iceberg.silver._debug_test (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4fe347-d814-4ef3-9f7b-0d4f986a8011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Phase3\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter-iceberg\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://lakehouse/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"SHOW CATALOGS\").show()  # ‚úÖ iceberg appears!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e10d8b80-ad7d-45f5-9257-9d55b9520ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Builder' object has no attribute 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Builder' object has no attribute 'stop'"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8d7c16-befe-4504-a676-a69c5161885f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetail-Lakehouse-Iceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://spark-master:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.iceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg.spark.SparkCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.iceberg.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.iceberg.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://iceberg-rest:8181\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.iceberg.warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://lakehouse/warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Lakehouse-Iceberg\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\")\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://lakehouse/warehouse\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e6d75c-c253-4ea0-867f-6c2d8a0bd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7be35a-28d4-4387-b1f9-e6f91258c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Lakehouse-Polaris\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "\n",
    "    .config(\"spark.driver.host\", \"jupyter\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "    # REQUIRED\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    # Polaris REST catalog\n",
    "    .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.polaris.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\")\n",
    "    .config(\"spark.sql.catalog.polaris.uri\", \"http://polaris:8181\")\n",
    "    .config(\"spark.sql.catalog.polaris.warehouse\", \"s3://lakehouse/warehouse\")\n",
    "\n",
    "    # MinIO (S3)\n",
    "    .config(\"spark.sql.catalog.polaris.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.access-key-id\", \"minio\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.secret-access-key\", \"minio123\")\n",
    "    .config(\"spark.sql.catalog.polaris.s3.path-style-access\", \"true\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedae0da-0f6c-42ec-aa0c-dd375e15b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Lakehouse-Nessie\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "\n",
    "    # Docker-safe networking\n",
    "    .config(\"spark.driver.host\", \"jupyter\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "    # Extensions (Iceberg + Nessie)\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "        \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    # Nessie catalog\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://lakehouse/warehouse\")\n",
    "\n",
    "    # S3 / MinIO\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"minio\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"minio123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "\n",
    "    # Hadoop S3A\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4084abe6-f6a0-4c1f-be76-ad569c4fb01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.extensions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525a4635-40b5-437b-aac0-9679fa3ab2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o57"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.java.lang.Class.forName(\n",
    "    \"org.apache.iceberg.spark.SparkCatalog\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3668366c-4391-4276-b0dc-856e29599afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o58"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.java.lang.Class.forName(\n",
    "    \"org.apache.iceberg.rest.RESTCatalog\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6365bfa-6f27-4c7d-a948-f7f3987cb7d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f8f485-ef9b-47e9-b489-5e65b0ff3089",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3328262775.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    spark version()# Check Spark version\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "spark version()\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Scala Version: {spark.sparkContext._gateway.jvm.scala.util.Properties.versionString()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd63f0f9-35df-483d-bdd3-7411ec48a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245f09b2-ced2-48dd-84cd-2bd4c08bcb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b525d1d-db1b-4738-aeae-a4a9b0c15ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark://spark-master:7077'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1606ef5c-be02-4e0f-8e76-1dc5ab687d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for JAR files...\n",
      "‚úÖ Found 3 JAR files:\n",
      "  ‚Ä¢ iceberg-aws-bundle-1.5.2.jar (29.1 MB)\n",
      "  ‚Ä¢ iceberg-spark-runtime-3.5_2.12-1.5.2.jar (39.7 MB)\n",
      "  ‚Ä¢ nessie-spark-extensions-3.5_2.12-0.84.0.jar (0.0 MB)\n",
      "\n",
      "============================================================\n",
      "üöÄ Creating Spark Session with full AWS configuration...\n",
      "============================================================\n",
      "‚úÖ Spark session created successfully!\n",
      "============================================================\n",
      "\n",
      "üìã Available Catalogs:\n",
      "============================================================\n",
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|nessie       |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "üóÇÔ∏è Creating test namespace and table...\n",
      "============================================================\n",
      "‚úÖ Namespace 'retail_test' created\n",
      "‚úÖ Table 'sample_products' created\n",
      "\n",
      "üìù Inserting sample data...\n",
      "‚úÖ Sample data inserted\n",
      "\n",
      "üìä Querying sample_products table:\n",
      "============================================================\n",
      "+----------+------------+-----------+------+--------------------------+\n",
      "|product_id|product_name|category   |price |created_at                |\n",
      "+----------+------------+-----------+------+--------------------------+\n",
      "|1         |Laptop      |Electronics|999.99|2026-01-28 16:18:52.080288|\n",
      "|1         |Laptop      |Electronics|999.99|2026-01-28 16:17:58.793587|\n",
      "|2         |Mouse       |Electronics|29.99 |2026-01-28 16:18:52.080421|\n",
      "|2         |Mouse       |Electronics|29.99 |2026-01-28 16:17:58.798094|\n",
      "|3         |Desk Chair  |Furniture  |199.99|2026-01-28 16:18:52.080517|\n",
      "|3         |Desk Chair  |Furniture  |199.99|2026-01-28 16:17:58.798236|\n",
      "|4         |Monitor     |Electronics|299.99|2026-01-28 16:18:52.080557|\n",
      "|4         |Monitor     |Electronics|299.99|2026-01-28 16:17:58.799052|\n",
      "|5         |Keyboard    |Electronics|79.99 |2026-01-28 16:18:52.080628|\n",
      "|5         |Keyboard    |Electronics|79.99 |2026-01-28 16:17:58.799267|\n",
      "+----------+------------+-----------+------+--------------------------+\n",
      "\n",
      "\n",
      "üìà Total products: 10\n",
      "\n",
      "üí∞ Products by Category:\n",
      "============================================================\n",
      "+-----------+-------------+---------+-----------+\n",
      "|category   |product_count|avg_price|total_value|\n",
      "+-----------+-------------+---------+-----------+\n",
      "|Electronics|8            |352.49   |2819.92    |\n",
      "|Furniture  |2            |199.99   |399.98     |\n",
      "+-----------+-------------+---------+-----------+\n",
      "\n",
      "\n",
      "üìú Table History (Iceberg Time Travel):\n",
      "============================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-28 16:18:03.115|4395964066385871325|NULL               |true               |\n",
      "|2026-01-28 16:18:52.395|6769498793626908395|4395964066385871325|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "üì∏ Table Snapshots:\n",
      "============================================================\n",
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                           |summary                                                                                                                                                                                                                                                                                              |\n",
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2026-01-28 16:18:03.115|4395964066385871325|NULL               |append   |s3a://lakehouse/warehouse/retail_test/sample_products_70169b58-4c01-4ddb-9a0b-88070de3447d/metadata/snap-4395964066385871325-1-d3161512-efd9-4cc0-8b4b-02e2329e70bf.avro|{spark.app.id -> app-20260128161736-0000, added-data-files -> 2, added-records -> 5, added-files-size -> 3053, changed-partition-count -> 1, total-records -> 5, total-files-size -> 3053, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2026-01-28 16:18:52.395|6769498793626908395|4395964066385871325|append   |s3a://lakehouse/warehouse/retail_test/sample_products_70169b58-4c01-4ddb-9a0b-88070de3447d/metadata/snap-6769498793626908395-1-0e81840c-84ca-4ed9-adbb-c04bf79f6fdc.avro|{spark.app.id -> app-20260128161736-0000, added-data-files -> 2, added-records -> 5, added-files-size -> 3053, changed-partition-count -> 1, total-records -> 10, total-files-size -> 6106, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+-------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üéâ SUCCESS! Your Retail Lakehouse is FULLY OPERATIONAL!\n",
      "============================================================\n",
      "\n",
      "‚úÖ What's working:\n",
      "  ‚Ä¢ ‚úì Spark 3.5.0\n",
      "  ‚Ä¢ ‚úì Apache Iceberg 1.5.2 (with time travel!)\n",
      "  ‚Ä¢ ‚úì Project Nessie 0.84.0 (catalog with Git-like versioning)\n",
      "  ‚Ä¢ ‚úì MinIO S3-compatible storage\n",
      "  ‚Ä¢ ‚úì AWS SDK properly configured\n",
      "  ‚Ä¢ ‚úì Data successfully written and queried\n",
      "\n",
      "üöÄ Advanced Features Available:\n",
      "  ‚Ä¢ Time Travel: Query historical data\n",
      "  ‚Ä¢ Schema Evolution: Add/remove columns safely\n",
      "  ‚Ä¢ Partition Evolution: Change partitioning without rewriting data\n",
      "  ‚Ä¢ Hidden Partitioning: No partition predicates needed in queries\n",
      "  ‚Ä¢ ACID Transactions: Full consistency guarantees\n",
      "  ‚Ä¢ Nessie Branches: Experiment with data changes safely\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Create your retail schema (customers, orders, inventory)\n",
      "  2. Set up NiFi for automated data ingestion\n",
      "  3. Configure Trino for interactive SQL queries\n",
      "  4. Build Superset dashboards for visualization\n",
      "  5. Explore Nessie branching for dev/staging/prod workflows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# ====== Configuration Variables ======\n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://lakehouse/warehouse\"\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"minio\"\n",
    "MINIO_SECRET_KEY = \"minio123\"\n",
    "AWS_REGION = \"us-east-1\"  # Required for AWS SDK\n",
    "\n",
    "# ====== JAR Configuration ======\n",
    "JARS_PATH = \"/home/jovyan/spark-jars\"\n",
    "\n",
    "# Check if JARs are present\n",
    "print(\"üîç Checking for JAR files...\")\n",
    "if os.path.exists(JARS_PATH):\n",
    "    jar_files = [f for f in os.listdir(JARS_PATH) if f.endswith('.jar')]\n",
    "    if jar_files:\n",
    "        print(f\"‚úÖ Found {len(jar_files)} JAR files:\")\n",
    "        for jar in sorted(jar_files):\n",
    "            jar_path = os.path.join(JARS_PATH, jar)\n",
    "            size_mb = os.path.getsize(jar_path) / (1024*1024)\n",
    "            print(f\"  ‚Ä¢ {jar} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No JAR files found in {JARS_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Directory {JARS_PATH} does not exist!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Build the JARs string\n",
    "jars_list = [\n",
    "    f\"{JARS_PATH}/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\",\n",
    "    f\"{JARS_PATH}/iceberg-aws-bundle-1.5.2.jar\",\n",
    "    f\"{JARS_PATH}/nessie-spark-extensions-3.5_2.12-0.84.0.jar\"\n",
    "]\n",
    "jars_string = \",\".join(jars_list)\n",
    "\n",
    "print(\"üöÄ Creating Spark Session with full AWS configuration...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ====== Build Spark Session with AWS Region ======\n",
    "try:\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Retail-Lakehouse-Iceberg-Nessie\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        \n",
    "        # Load JARs\n",
    "        .config(\"spark.jars\", jars_string)\n",
    "        .config(\"spark.driver.extraClassPath\", f\"{JARS_PATH}/*\")\n",
    "        .config(\"spark.executor.extraClassPath\", f\"{JARS_PATH}/*\")\n",
    "        \n",
    "        # Enable Iceberg + Nessie SQL extensions\n",
    "        .config(\"spark.sql.extensions\",\n",
    "                \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "                \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "        \n",
    "        # Configure Nessie catalog\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "        .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .config(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "        .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        \n",
    "        # CRITICAL: AWS Region configuration\n",
    "        .config(\"spark.sql.catalog.nessie.client.region\", AWS_REGION)\n",
    "        \n",
    "        # S3/MinIO configuration for Iceberg\n",
    "        .config(\"spark.sql.catalog.nessie.s3.endpoint\", MINIO_ENDPOINT)\n",
    "        .config(\"spark.sql.catalog.nessie.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
    "        .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
    "        .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "        \n",
    "        # Hadoop S3A configuration\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        \n",
    "        # CRITICAL: AWS SDK configuration (prevents region errors)\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "        \n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Spark session created successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create Spark session: {e}\")\n",
    "    raise\n",
    "\n",
    "# ====== Test Catalogs ======\n",
    "print(\"\\nüìã Available Catalogs:\")\n",
    "print(\"=\"*60)\n",
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n",
    "\n",
    "# ====== Create Test Namespace and Table ======\n",
    "print(\"\\nüóÇÔ∏è Creating test namespace and table...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Create namespace\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.retail_test\")\n",
    "    print(\"‚úÖ Namespace 'retail_test' created\")\n",
    "    \n",
    "    # Create table\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS nessie.retail_test.sample_products (\n",
    "            product_id INT,\n",
    "            product_name STRING,\n",
    "            category STRING,\n",
    "            price DECIMAL(10,2),\n",
    "            created_at TIMESTAMP\n",
    "        ) USING iceberg\n",
    "        TBLPROPERTIES (\n",
    "            'format-version'='2',\n",
    "            'write.parquet.compression-codec'='snappy'\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Table 'sample_products' created\")\n",
    "    \n",
    "    # Insert sample data\n",
    "    print(\"\\nüìù Inserting sample data...\")\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO nessie.retail_test.sample_products VALUES\n",
    "        (1, 'Laptop', 'Electronics', 999.99, current_timestamp()),\n",
    "        (2, 'Mouse', 'Electronics', 29.99, current_timestamp()),\n",
    "        (3, 'Desk Chair', 'Furniture', 199.99, current_timestamp()),\n",
    "        (4, 'Monitor', 'Electronics', 299.99, current_timestamp()),\n",
    "        (5, 'Keyboard', 'Electronics', 79.99, current_timestamp())\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Sample data inserted\")\n",
    "    \n",
    "    # Query the table\n",
    "    print(\"\\nüìä Querying sample_products table:\")\n",
    "    print(\"=\"*60)\n",
    "    spark.sql(\"SELECT * FROM nessie.retail_test.sample_products ORDER BY product_id\").show(truncate=False)\n",
    "    \n",
    "    # Show count\n",
    "    count_df = spark.sql(\"SELECT COUNT(*) as total_products FROM nessie.retail_test.sample_products\")\n",
    "    total = count_df.collect()[0]['total_products']\n",
    "    print(f\"\\nüìà Total products: {total}\")\n",
    "    \n",
    "    # Show aggregations\n",
    "    print(\"\\nüí∞ Products by Category:\")\n",
    "    print(\"=\"*60)\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            category,\n",
    "            COUNT(*) as product_count,\n",
    "            ROUND(AVG(price), 2) as avg_price,\n",
    "            ROUND(SUM(price), 2) as total_value\n",
    "        FROM nessie.retail_test.sample_products\n",
    "        GROUP BY category\n",
    "        ORDER BY total_value DESC\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    # Show table history (Iceberg feature)\n",
    "    print(\"\\nüìú Table History (Iceberg Time Travel):\")\n",
    "    print(\"=\"*60)\n",
    "    spark.sql(\"SELECT * FROM nessie.retail_test.sample_products.history\").show(truncate=False)\n",
    "    \n",
    "    # Show snapshots\n",
    "    print(\"\\nüì∏ Table Snapshots:\")\n",
    "    print(\"=\"*60)\n",
    "    spark.sql(\"SELECT * FROM nessie.retail_test.sample_products.snapshots\").show(truncate=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ SUCCESS! Your Retail Lakehouse is FULLY OPERATIONAL!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n‚úÖ What's working:\")\n",
    "    print(\"  ‚Ä¢ ‚úì Spark 3.5.0\")\n",
    "    print(\"  ‚Ä¢ ‚úì Apache Iceberg 1.5.2 (with time travel!)\")\n",
    "    print(\"  ‚Ä¢ ‚úì Project Nessie 0.84.0 (catalog with Git-like versioning)\")\n",
    "    print(\"  ‚Ä¢ ‚úì MinIO S3-compatible storage\")\n",
    "    print(\"  ‚Ä¢ ‚úì AWS SDK properly configured\")\n",
    "    print(\"  ‚Ä¢ ‚úì Data successfully written and queried\")\n",
    "    print(\"\\nüöÄ Advanced Features Available:\")\n",
    "    print(\"  ‚Ä¢ Time Travel: Query historical data\")\n",
    "    print(\"  ‚Ä¢ Schema Evolution: Add/remove columns safely\")\n",
    "    print(\"  ‚Ä¢ Partition Evolution: Change partitioning without rewriting data\")\n",
    "    print(\"  ‚Ä¢ Hidden Partitioning: No partition predicates needed in queries\")\n",
    "    print(\"  ‚Ä¢ ACID Transactions: Full consistency guarantees\")\n",
    "    print(\"  ‚Ä¢ Nessie Branches: Experiment with data changes safely\")\n",
    "    print(\"\\nüéØ Next Steps:\")\n",
    "    print(\"  1. Create your retail schema (customers, orders, inventory)\")\n",
    "    print(\"  2. Set up NiFi for automated data ingestion\")\n",
    "    print(\"  3. Configure Trino for interactive SQL queries\")\n",
    "    print(\"  4. Build Superset dashboards for visualization\")\n",
    "    print(\"  5. Explore Nessie branching for dev/staging/prod workflows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during table operations: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"1. Check Nessie logs: docker logs nessie\")\n",
    "    print(\"2. Check MinIO access: docker exec mc mc ls local/lakehouse\")\n",
    "    print(\"3. Verify network connectivity between containers\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ead1ee-eff4-4195-be1b-75bb5bbfe5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.extensions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e38ba7-13de-419c-9383-23d72c6fa78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|nessie       |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0922b7-b9da-488a-8043-fad0fd3c9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Iceberg-Nessie-Retail\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "\n",
    "    # =========================\n",
    "    # Iceberg Extensions\n",
    "    # =========================\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Nessie Catalog\n",
    "    # =========================\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "\n",
    "    # =========================\n",
    "    # Warehouse (MinIO)\n",
    "    # =========================\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/iceberg\")\n",
    "\n",
    "    # =========================\n",
    "    # S3A ‚Üí MinIO\n",
    "    # =========================\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    # =========================\n",
    "    # üö® AWS SDK FIX (CRITICAL)\n",
    "    # =========================\n",
    "    .config(\"spark.hadoop.aws.region\", \"us-east-1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9480ac7-05a6-40fa-a98e-9feb7a44a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|       nessie|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd532801-bb80-4202-8b36-985f3c0d3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 10).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7814a6a1-d19a-45c6-a7a8-3cd6ec4c80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üè™ RETAIL LAKEHOUSE - DATA PROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "üîß Creating Spark Session with Nessie catalog...\n",
      "‚úÖ Spark session created with Nessie catalog!\n",
      "\n",
      "üìÅ Creating lakehouse namespaces (Bronze/Silver/Gold)...\n",
      "‚úÖ Namespaces created: bronze, silver, gold\n",
      "\n",
      "======================================================================\n",
      "[1/8] üì• LOADING RAW DATA\n",
      "======================================================================\n",
      "Loading from: /opt/spark-data/online_retail_combined.csv\n",
      "‚úÖ Loaded 1,067,371 rows\n",
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Customer ID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- fiscal_year: string (nullable = true)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[2/8] üîç DATA QUALITY CHECKS\n",
      "======================================================================\n",
      "\n",
      "üìä Null value counts:\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|fiscal_year|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "|      0|        0|       4382|       0|          0|    0|     243007|      0|          0|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+-----------+\n",
      "\n",
      "\n",
      "üìã Total rows: 1,067,371\n",
      "üìã Distinct rows: 1,055,238\n",
      "üìã Duplicates: 12,133\n",
      "\n",
      "======================================================================\n",
      "[3/8] üßπ DATA CLEANING\n",
      "======================================================================\n",
      "‚úÖ After cleaning: 793,609 rows\n",
      "   Removed: 273,762 rows (25.6%)\n",
      "\n",
      "======================================================================\n",
      "[4/8] ‚öôÔ∏è  DATA TRANSFORMATION\n",
      "======================================================================\n",
      "‚úÖ Added columns:\n",
      "   ‚Ä¢ TotalAmount (Quantity √ó Price)\n",
      "   ‚Ä¢ Year, Month, DayOfWeek, Hour\n",
      "   ‚Ä¢ CustomerID (renamed from Customer ID)\n",
      "+-------+---------+----------------------------------+--------+-------------------+-----+--------------+-----------+------------------+----+-----+---------+----+----------+\n",
      "|Invoice|StockCode|Description                       |Quantity|InvoiceDate        |Price|Country       |fiscal_year|TotalAmount       |Year|Month|DayOfWeek|Hour|CustomerID|\n",
      "+-------+---------+----------------------------------+--------+-------------------+-----+--------------+-----------+------------------+----+-----+---------+----+----------+\n",
      "|489439 |22064    |PINK DOUGHNUT TRINKET POT         |12      |2009-12-01 09:28:00|1.65 |France        |2009-2010  |19.799999999999997|2009|12   |3        |9   |12682.0   |\n",
      "|489488 |20972    |PINK CREAM FELT CRAFT TRINKET BOX |3       |2009-12-01 10:59:00|1.25 |United Kingdom|2009-2010  |3.75              |2009|12   |3        |10  |17238.0   |\n",
      "|489526 |20914    |SET/5 RED SPOTTY LID GLASS BOWLS  |12      |2009-12-01 11:50:00|2.95 |Germany       |2009-2010  |35.400000000000006|2009|12   |3        |11  |12533.0   |\n",
      "|489536 |21918    |SET 12 KIDS COLOUR  CHALK STICKS  |2       |2009-12-01 12:13:00|0.42 |United Kingdom|2009-2010  |0.84              |2009|12   |3        |12  |16393.0   |\n",
      "|489545 |22273    |FELTCRAFT DOLL MOLLY              |1       |2009-12-01 12:22:00|2.95 |United Kingdom|2009-2010  |2.95              |2009|12   |3        |12  |17804.0   |\n",
      "+-------+---------+----------------------------------+--------+-------------------+-----+--------------+-----------+------------------+----+-----+---------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[5/8] üë• CUSTOMER METRICS\n",
      "======================================================================\n",
      "‚úÖ Customer metrics aggregated\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|CustomerID|TotalOrders|TotalSpent        |AvgOrderValue     |UniqueInvoices|FirstPurchaseDate  |LastPurchaseDate   |DaysSinceFirstPurchase|\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "|18102.0   |1058       |608821.6499999999 |575.4457939508505 |145           |2009-12-01 09:24:00|2011-12-09 11:50:00|738                   |\n",
      "|14646.0   |3849       |528602.52         |137.33502727981295|151           |2009-12-02 16:52:00|2011-12-08 12:12:00|736                   |\n",
      "|14156.0   |4041       |313759.82000000007|77.64410294481566 |156           |2009-12-01 12:30:00|2011-11-30 10:54:00|729                   |\n",
      "|14911.0   |11238      |295832.39         |26.32429168891262 |398           |2009-12-01 11:41:00|2011-12-08 15:54:00|737                   |\n",
      "|17450.0   |423        |246813.09000000003|583.4824822695036 |51            |2010-09-27 16:59:00|2011-12-01 13:29:00|430                   |\n",
      "+----------+-----------+------------------+------------------+--------------+-------------------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[6/8] üì¶ PRODUCT METRICS\n",
      "======================================================================\n",
      "‚úÖ Product metrics calculated\n",
      "+---------+----------------------------------+-----------------+------------------+---------------+------------------+\n",
      "|StockCode|Description                       |TotalQuantitySold|TotalRevenue      |UniqueCustomers|AvgPrice          |\n",
      "+---------+----------------------------------+-----------------+------------------+---------------+------------------+\n",
      "|22423    |REGENCY CAKESTAND 3 TIER          |24858            |285992.3500000001 |1314           |12.459903169014098|\n",
      "|85123A   |WHITE HANGING HEART T-LIGHT HOLDER|93520            |251731.2600000002 |1490           |2.8695120046847347|\n",
      "|23843    |PAPER CRAFT , LITTLE BIRDIE       |80995            |168469.6          |1              |2.08              |\n",
      "|M        |Manual                            |9501             |151951.91999999993|438            |209.37911301859788|\n",
      "|85099B   |JUMBO BAG RED RETROSPOT           |75597            |136684.79000000036|860            |1.9725427509293596|\n",
      "+---------+----------------------------------+-----------------+------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[7/8] üìä RFM ANALYSIS (Recency, Frequency, Monetary)\n",
      "======================================================================\n",
      "Reference date for recency: 2011-12-09 12:50:00\n",
      "‚úÖ RFM analysis complete with customer segmentation\n",
      "+-------------------+-----+\n",
      "|    CustomerSegment|count|\n",
      "+-------------------+-----+\n",
      "|          Champions| 1746|\n",
      "|            At Risk| 1090|\n",
      "|   Recent Customers| 1071|\n",
      "|    Loyal Customers| 1065|\n",
      "|Potential Loyalists|  906|\n",
      "+-------------------+-----+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[8/8] üíæ SAVING TO NESSIE ICEBERG LAKEHOUSE\n",
      "======================================================================\n",
      "\n",
      "ü•â Writing BRONZE layer (partitioned by Year, Month)...\n",
      "‚úÖ nessie.bronze.retail_transactions created\n",
      "\n",
      "ü•à Writing SILVER layer (business metrics)...\n",
      "‚úÖ nessie.silver.customer_metrics created\n",
      "‚úÖ nessie.silver.product_metrics created\n",
      "‚úÖ nessie.silver.rfm_analysis created\n",
      "\n",
      "ü•á Writing GOLD layer (curated datasets)...\n",
      "‚úÖ nessie.gold.top_customers created\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PIPELINE COMPLETE - VERIFYING RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìã Tables created in Nessie catalog:\n",
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|bronze   |retail_transactions|false      |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "+---------+----------------+-----------+\n",
      "|namespace|tableName       |isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|silver   |customer_metrics|false      |\n",
      "|silver   |product_metrics |false      |\n",
      "|silver   |rfm_analysis    |false      |\n",
      "+---------+----------------+-----------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|tableName    |isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|gold     |top_customers|false      |\n",
      "+---------+-------------+-----------+\n",
      "\n",
      "\n",
      "üìä Record counts:\n",
      "  ‚Ä¢ Bronze (transactions): 793,609\n",
      "  ‚Ä¢ Silver (customers): 5,878\n",
      "  ‚Ä¢ Silver (products): 5,315\n",
      "  ‚Ä¢ Silver (RFM): 5,878\n",
      "  ‚Ä¢ Gold (top customers): 2,811\n",
      "\n",
      "üîç Sample query - Top 5 products by revenue:\n",
      "+---------+----------------------------------+------------------+-----------------+---------------+\n",
      "|StockCode|Description                       |TotalRevenue      |TotalQuantitySold|UniqueCustomers|\n",
      "+---------+----------------------------------+------------------+-----------------+---------------+\n",
      "|22423    |REGENCY CAKESTAND 3 TIER          |285992.3500000001 |24858            |1314           |\n",
      "|85123A   |WHITE HANGING HEART T-LIGHT HOLDER|251731.2600000002 |93520            |1490           |\n",
      "|23843    |PAPER CRAFT , LITTLE BIRDIE       |168469.6          |80995            |1              |\n",
      "|M        |Manual                            |151951.91999999993|9501             |438            |\n",
      "|85099B   |JUMBO BAG RED RETROSPOT           |136684.79000000036|75597            |860            |\n",
      "+---------+----------------------------------+------------------+-----------------+---------------+\n",
      "\n",
      "\n",
      "üîç Customer segmentation distribution:\n",
      "+-------------------+-------------+-------------+----------+\n",
      "|CustomerSegment    |CustomerCount|TotalRevenue |AvgRevenue|\n",
      "+-------------------+-------------+-------------+----------+\n",
      "|Champions          |1746         |1.266517373E7|7253.82   |\n",
      "|Loyal Customers    |1065         |2169181.94   |2036.79   |\n",
      "|Potential Loyalists|906          |1377181.06   |1520.07   |\n",
      "|Recent Customers   |1071         |927751.04    |866.25    |\n",
      "|At Risk            |1090         |546172.85    |501.08    |\n",
      "+-------------------+-------------+-------------+----------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéâ RETAIL LAKEHOUSE PIPELINE SUCCESSFULLY COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "‚úÖ What was created:\n",
      "  ‚Ä¢ Bronze layer: Raw transaction data (partitioned by Year/Month)\n",
      "  ‚Ä¢ Silver layer: Customer, product, and RFM metrics\n",
      "  ‚Ä¢ Gold layer: Top customers for analytics\n",
      "\n",
      "üìä Lakehouse Features Available:\n",
      "  ‚Ä¢ Time Travel: Query historical data snapshots\n",
      "  ‚Ä¢ Schema Evolution: Add columns without breaking queries\n",
      "  ‚Ä¢ ACID Transactions: Full consistency guarantees\n",
      "  ‚Ä¢ Partition Evolution: Change partitioning strategy\n",
      "  ‚Ä¢ Nessie Branching: Test changes in dev branches\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Query your data: spark.sql('SELECT * FROM nessie.silver.rfm_analysis')\n",
      "  2. Create Trino views for SQL access\n",
      "  3. Build Superset dashboards\n",
      "  4. Explore Nessie branches for experimentation\n",
      "  5. Implement ML models on RFM segments\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üè™ RETAIL LAKEHOUSE - DATA PROCESSING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ====== Configuration Variables ======\n",
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://lakehouse/warehouse\"\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"minio\"\n",
    "MINIO_SECRET_KEY = \"minio123\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "JARS_PATH = \"/home/jovyan/spark-jars\"\n",
    "\n",
    "# ====== Build Spark Session ======\n",
    "print(\"\\nüîß Creating Spark Session with Nessie catalog...\")\n",
    "\n",
    "jars_list = [\n",
    "    f\"{JARS_PATH}/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\",\n",
    "    f\"{JARS_PATH}/iceberg-aws-bundle-1.5.2.jar\",\n",
    "    f\"{JARS_PATH}/nessie-spark-extensions-3.5_2.12-0.84.0.jar\"\n",
    "]\n",
    "jars_string = \",\".join(jars_list)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Retail-Data-Pipeline\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.jars\", jars_string)\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{JARS_PATH}/*\")\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{JARS_PATH}/*\")\n",
    "    .config(\"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .config(\"spark.sql.catalog.nessie.client.region\", AWS_REGION)\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark session created with Nessie catalog!\")\n",
    "\n",
    "# ====== Create Namespaces ======\n",
    "print(\"\\nüìÅ Creating lakehouse namespaces (Bronze/Silver/Gold)...\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.bronze\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.gold\")\n",
    "print(\"‚úÖ Namespaces created: bronze, silver, gold\")\n",
    "\n",
    "# ====== STEP 1: LOAD RAW DATA ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[1/8] üì• LOADING RAW DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if file exists\n",
    "data_path = \"/opt/spark-data/online_retail_combined.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"Loading from: {data_path}\")\n",
    "    df_raw = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "    row_count = df_raw.count()\n",
    "    print(f\"‚úÖ Loaded {row_count:,} rows\")\n",
    "    df_raw.printSchema()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  File not found: {data_path}\")\n",
    "    print(\"Creating sample data instead...\")\n",
    "    \n",
    "    # Create sample data matching the expected schema\n",
    "    schema = StructType([\n",
    "        StructField(\"Invoice\", StringType(), True),\n",
    "        StructField(\"StockCode\", StringType(), True),\n",
    "        StructField(\"Description\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"InvoiceDate\", StringType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    sample_data = [\n",
    "        (\"536365\", \"85123A\", \"WHITE HANGING HEART T-LIGHT HOLDER\", 6, \"2010-12-01 08:26:00\", 2.55, \"17850\", \"United Kingdom\"),\n",
    "        (\"536365\", \"71053\", \"WHITE METAL LANTERN\", 6, \"2010-12-01 08:26:00\", 3.39, \"17850\", \"United Kingdom\"),\n",
    "        (\"536365\", \"84406B\", \"CREAM CUPID HEARTS COAT HANGER\", 8, \"2010-12-01 08:26:00\", 2.75, \"17850\", \"United Kingdom\"),\n",
    "        (\"536366\", \"22633\", \"HAND WARMER UNION JACK\", 6, \"2010-12-01 08:28:00\", 1.85, \"17850\", \"United Kingdom\"),\n",
    "        (\"536367\", \"84879\", \"ASSORTED COLOUR BIRD ORNAMENT\", 32, \"2010-12-01 08:34:00\", 1.69, \"13047\", \"United Kingdom\"),\n",
    "        (\"536367\", \"22745\", \"POPPY'S PLAYHOUSE BEDROOM\", 6, \"2010-12-01 08:34:00\", 2.10, \"13047\", \"United Kingdom\"),\n",
    "        (\"536368\", \"22960\", \"JAM MAKING SET WITH JARS\", 24, \"2010-12-01 08:34:00\", 1.45, \"13047\", \"United Kingdom\"),\n",
    "        (\"536369\", \"21756\", \"BATH BUILDING BLOCK WORD\", 3, \"2010-12-01 08:35:00\", 5.95, \"13047\", \"United Kingdom\"),\n",
    "        (\"536370\", \"22086\", \"PAPER CHAIN KIT 50'S CHRISTMAS\", 80, \"2010-12-01 08:45:00\", 2.55, \"12583\", \"France\"),\n",
    "        (\"536371\", \"22866\", \"HAND WARMER SCOTTY DOG DESIGN\", 12, \"2010-12-01 09:00:00\", 2.10, \"17850\", \"United Kingdom\")\n",
    "    ]\n",
    "    \n",
    "    df_raw = spark.createDataFrame(sample_data, schema)\n",
    "    row_count = df_raw.count()\n",
    "    print(f\"‚úÖ Created {row_count:,} sample rows\")\n",
    "\n",
    "# ====== STEP 2: DATA QUALITY CHECKS ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[2/8] üîç DATA QUALITY CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "print(\"\\nüìä Null value counts:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Check for duplicates\n",
    "total_rows = df_raw.count()\n",
    "distinct_rows = df_raw.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"\\nüìã Total rows: {total_rows:,}\")\n",
    "print(f\"üìã Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"üìã Duplicates: {duplicates:,}\")\n",
    "\n",
    "# ====== STEP 3: DATA CLEANING ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[3/8] üßπ DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"Invoice\").isNotNull()) \\\n",
    "    .filter(col(\"StockCode\").isNotNull()) \\\n",
    "    .filter(col(\"Quantity\") > 0) \\\n",
    "    .filter(col(\"Price\") > 0) \\\n",
    "    .filter(col(\"Customer ID\").isNotNull()) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "cleaned_count = df_cleaned.count()\n",
    "removed = total_rows - cleaned_count\n",
    "print(f\"‚úÖ After cleaning: {cleaned_count:,} rows\")\n",
    "print(f\"   Removed: {removed:,} rows ({(removed/total_rows*100):.1f}%)\")\n",
    "\n",
    "# ====== STEP 4: DATA TRANSFORMATION ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[4/8] ‚öôÔ∏è  DATA TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_transformed = df_cleaned \\\n",
    "    .withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"Hour\", hour(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"Customer ID\").cast(\"string\")) \\\n",
    "    .drop(\"Customer ID\")\n",
    "\n",
    "print(\"‚úÖ Added columns:\")\n",
    "print(\"   ‚Ä¢ TotalAmount (Quantity √ó Price)\")\n",
    "print(\"   ‚Ä¢ Year, Month, DayOfWeek, Hour\")\n",
    "print(\"   ‚Ä¢ CustomerID (renamed from Customer ID)\")\n",
    "\n",
    "df_transformed.show(5, truncate=False)\n",
    "\n",
    "# ====== STEP 5: CUSTOMER METRICS ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[5/8] üë• CUSTOMER METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_customer_metrics = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        count(\"Invoice\").alias(\"TotalOrders\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalSpent\"),\n",
    "        avg(\"TotalAmount\").alias(\"AvgOrderValue\"),\n",
    "        countDistinct(\"Invoice\").alias(\"UniqueInvoices\"),\n",
    "        min(\"InvoiceDate\").alias(\"FirstPurchaseDate\"),\n",
    "        max(\"InvoiceDate\").alias(\"LastPurchaseDate\")\n",
    "    ) \\\n",
    "    .withColumn(\"DaysSinceFirstPurchase\", \n",
    "                datediff(col(\"LastPurchaseDate\"), col(\"FirstPurchaseDate\")))\n",
    "\n",
    "print(\"‚úÖ Customer metrics aggregated\")\n",
    "df_customer_metrics.orderBy(desc(\"TotalSpent\")).show(5, truncate=False)\n",
    "\n",
    "# ====== STEP 6: PRODUCT METRICS ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[6/8] üì¶ PRODUCT METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_product_metrics = df_transformed.groupBy(\"StockCode\", \"Description\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalRevenue\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"UniqueCustomers\"),\n",
    "        avg(\"Price\").alias(\"AvgPrice\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"TotalRevenue\"))\n",
    "\n",
    "print(\"‚úÖ Product metrics calculated\")\n",
    "df_product_metrics.show(5, truncate=False)\n",
    "\n",
    "# ====== STEP 7: RFM ANALYSIS ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[7/8] üìä RFM ANALYSIS (Recency, Frequency, Monetary)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the latest date in dataset\n",
    "max_date = df_transformed.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "print(f\"Reference date for recency: {max_date}\")\n",
    "\n",
    "df_rfm = df_transformed.groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Invoice\").alias(\"Frequency\"),\n",
    "        sum(\"TotalAmount\").alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "# Calculate RFM scores (1-5 scale)\n",
    "rfm_quantiles = df_rfm.approxQuantile([\"Recency\", \"Frequency\", \"Monetary\"], \n",
    "                                       [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "\n",
    "df_rfm_scored = df_rfm \\\n",
    "    .withColumn(\"R_Score\", \n",
    "                when(col(\"Recency\") <= rfm_quantiles[0][0], 5)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][1], 4)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][2], 3)\n",
    "                .when(col(\"Recency\") <= rfm_quantiles[0][3], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"F_Score\",\n",
    "                when(col(\"Frequency\") >= rfm_quantiles[1][3], 5)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][2], 4)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][1], 3)\n",
    "                .when(col(\"Frequency\") >= rfm_quantiles[1][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"M_Score\",\n",
    "                when(col(\"Monetary\") >= rfm_quantiles[2][3], 5)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][2], 4)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][1], 3)\n",
    "                .when(col(\"Monetary\") >= rfm_quantiles[2][0], 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"RFM_Score\", \n",
    "                concat(col(\"R_Score\"), col(\"F_Score\"), col(\"M_Score\"))) \\\n",
    "    .withColumn(\"CustomerSegment\",\n",
    "                when(col(\"RFM_Score\") >= \"444\", \"Champions\")\n",
    "                .when(col(\"RFM_Score\") >= \"344\", \"Loyal Customers\")\n",
    "                .when(col(\"RFM_Score\") >= \"244\", \"Potential Loyalists\")\n",
    "                .when(col(\"RFM_Score\") >= \"143\", \"Recent Customers\")\n",
    "                .when(col(\"RFM_Score\") >= \"111\", \"At Risk\")\n",
    "                .otherwise(\"Lost\"))\n",
    "\n",
    "print(\"‚úÖ RFM analysis complete with customer segmentation\")\n",
    "df_rfm_scored.groupBy(\"CustomerSegment\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "# ====== STEP 8: SAVE TO ICEBERG LAKEHOUSE ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[8/8] üíæ SAVING TO NESSIE ICEBERG LAKEHOUSE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BRONZE LAYER - Raw cleaned data\n",
    "print(\"\\nü•â Writing BRONZE layer (partitioned by Year, Month)...\")\n",
    "df_transformed.writeTo(\"nessie.bronze.retail_transactions\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"Year\", \"Month\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .createOrReplace()\n",
    "print(\"‚úÖ nessie.bronze.retail_transactions created\")\n",
    "\n",
    "# SILVER LAYER - Aggregated metrics\n",
    "print(\"\\nü•à Writing SILVER layer (business metrics)...\")\n",
    "\n",
    "df_customer_metrics.writeTo(\"nessie.silver.customer_metrics\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .createOrReplace()\n",
    "print(\"‚úÖ nessie.silver.customer_metrics created\")\n",
    "\n",
    "df_product_metrics.writeTo(\"nessie.silver.product_metrics\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .createOrReplace()\n",
    "print(\"‚úÖ nessie.silver.product_metrics created\")\n",
    "\n",
    "df_rfm_scored.writeTo(\"nessie.silver.rfm_analysis\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .createOrReplace()\n",
    "print(\"‚úÖ nessie.silver.rfm_analysis created\")\n",
    "\n",
    "# GOLD LAYER - Top customers (for dashboards/ML)\n",
    "print(\"\\nü•á Writing GOLD layer (curated datasets)...\")\n",
    "\n",
    "df_top_customers = df_rfm_scored \\\n",
    "    .filter(col(\"CustomerSegment\").isin([\"Champions\", \"Loyal Customers\"])) \\\n",
    "    .select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \n",
    "            \"RFM_Score\", \"CustomerSegment\")\n",
    "\n",
    "df_top_customers.writeTo(\"nessie.gold.top_customers\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .createOrReplace()\n",
    "print(\"‚úÖ nessie.gold.top_customers created\")\n",
    "\n",
    "# ====== VERIFICATION ======\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE COMPLETE - VERIFYING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show all tables\n",
    "print(\"\\nüìã Tables created in Nessie catalog:\")\n",
    "spark.sql(\"SHOW TABLES IN nessie.bronze\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES IN nessie.silver\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES IN nessie.gold\").show(truncate=False)\n",
    "\n",
    "# Show record counts\n",
    "print(\"\\nüìä Record counts:\")\n",
    "bronze_count = spark.sql(\"SELECT COUNT(*) as count FROM nessie.bronze.retail_transactions\").collect()[0]['count']\n",
    "print(f\"  ‚Ä¢ Bronze (transactions): {bronze_count:,}\")\n",
    "\n",
    "customer_count = spark.sql(\"SELECT COUNT(*) as count FROM nessie.silver.customer_metrics\").collect()[0]['count']\n",
    "print(f\"  ‚Ä¢ Silver (customers): {customer_count:,}\")\n",
    "\n",
    "product_count = spark.sql(\"SELECT COUNT(*) as count FROM nessie.silver.product_metrics\").collect()[0]['count']\n",
    "print(f\"  ‚Ä¢ Silver (products): {product_count:,}\")\n",
    "\n",
    "rfm_count = spark.sql(\"SELECT COUNT(*) as count FROM nessie.silver.rfm_analysis\").collect()[0]['count']\n",
    "print(f\"  ‚Ä¢ Silver (RFM): {rfm_count:,}\")\n",
    "\n",
    "gold_count = spark.sql(\"SELECT COUNT(*) as count FROM nessie.gold.top_customers\").collect()[0]['count']\n",
    "print(f\"  ‚Ä¢ Gold (top customers): {gold_count:,}\")\n",
    "\n",
    "# Sample queries\n",
    "print(\"\\nüîç Sample query - Top 5 products by revenue:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        StockCode,\n",
    "        Description,\n",
    "        TotalRevenue,\n",
    "        TotalQuantitySold,\n",
    "        UniqueCustomers\n",
    "    FROM nessie.silver.product_metrics\n",
    "    ORDER BY TotalRevenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\nüîç Customer segmentation distribution:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CustomerSegment,\n",
    "        COUNT(*) as CustomerCount,\n",
    "        ROUND(SUM(Monetary), 2) as TotalRevenue,\n",
    "        ROUND(AVG(Monetary), 2) as AvgRevenue\n",
    "    FROM nessie.silver.rfm_analysis\n",
    "    GROUP BY CustomerSegment\n",
    "    ORDER BY TotalRevenue DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ RETAIL LAKEHOUSE PIPELINE SUCCESSFULLY COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ What was created:\")\n",
    "print(\"  ‚Ä¢ Bronze layer: Raw transaction data (partitioned by Year/Month)\")\n",
    "print(\"  ‚Ä¢ Silver layer: Customer, product, and RFM metrics\")\n",
    "print(\"  ‚Ä¢ Gold layer: Top customers for analytics\")\n",
    "print(\"\\nüìä Lakehouse Features Available:\")\n",
    "print(\"  ‚Ä¢ Time Travel: Query historical data snapshots\")\n",
    "print(\"  ‚Ä¢ Schema Evolution: Add columns without breaking queries\")\n",
    "print(\"  ‚Ä¢ ACID Transactions: Full consistency guarantees\")\n",
    "print(\"  ‚Ä¢ Partition Evolution: Change partitioning strategy\")\n",
    "print(\"  ‚Ä¢ Nessie Branching: Test changes in dev branches\")\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  1. Query your data: spark.sql('SELECT * FROM nessie.silver.rfm_analysis')\")\n",
    "print(\"  2. Create Trino views for SQL access\")\n",
    "print(\"  3. Build Superset dashboards\")\n",
    "print(\"  4. Explore Nessie branches for experimentation\")\n",
    "print(\"  5. Implement ML models on RFM segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1e784-bead-4105-bc22-167e6df0a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
